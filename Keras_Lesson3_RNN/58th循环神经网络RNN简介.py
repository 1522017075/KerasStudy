"""
    RNN:
        循环神经网络
            一个序列当前的输出 与 前面的输出 也有关系

                具体的表现形式为网络会对前面的信息进行记忆并且应用于当前输出的计算中, 既隐藏层之间的节点不再无连接
            而是有连接的, 并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出.
    LSTM网络:
        Long Short Term 网络 一般叫做 LSTM, 是一种 RNN 的特殊类型, 可以学习长期依赖信息.
        在很多问题, LSTM 都取得很大成功, 并有广泛应用, 是 RNN 事实上的标准

        LSTM 的窍门在于拥有一个固定权值为 1 的自连接, 以及一个线性激活函数, 因此其局部偏导始终为 1
        这样误差就能在时间步中传递, 而不会消失或爆炸

        LSTM 通过 门 对通过的信息进行控制, 门 是一种让信息选择式通过的方法, LSTM 通过门可以让信息不通过/完全通过/通过一部分

    GRU 门限循环单元:
        与 LSTM 相比, GRU 结构更简单, 它有一个更新门, 比 LSTM 结构简单, 计算少, 效果差不多

    Keras 中的实现:
        layers.LSTM()
        layers.GRU()

        输入是三维序列(batch, 评论长度, embeding 编码向量维度), 输出是二维(batch, 是/否)
"""